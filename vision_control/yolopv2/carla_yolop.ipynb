{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import socket\n",
    "import struct\n",
    "from sort import Sort\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "from waypoint_extractor import WaypointExtractor, Waypoint\n",
    "\n",
    "import carla\n",
    "import queue\n",
    "import random\n",
    "\n",
    "from utils.utils import (\n",
    "    time_synchronized, select_device, increment_path,\n",
    "    scale_coords, non_max_suppression, split_for_trace_model,\n",
    "    driving_area_mask, lane_line_mask, plot_one_box, show_seg_result,\n",
    "    AverageMeter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CARLACamera:\n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.world = None\n",
    "        self.vehicle = None\n",
    "        self.camera_rgb = None\n",
    "        self.camera_depth = None\n",
    "        self._image_queue = queue.Queue()\n",
    "        self._depth_queue = queue.Queue()\n",
    "        self.rgb_image = None\n",
    "        self.depth_array = None\n",
    "        \n",
    "    def image_callback(self, image):\n",
    "        \"\"\"Callback for RGB camera data\"\"\"\n",
    "        array = np.frombuffer(image.raw_data, dtype=np.dtype(\"uint8\"))\n",
    "        array = np.reshape(array, (image.height, image.width, 4))\n",
    "        array = array[:, :, :3]  # Remove alpha channel\n",
    "        array = array[:, :, ::-1]  # Convert RGBA to BGR\n",
    "        self.rgb_image = array\n",
    "        self._image_queue.put(array)\n",
    "\n",
    "    def depth_callback(self, image):\n",
    "        \"\"\"Callback for depth camera data\"\"\"\n",
    "        array = np.frombuffer(image.raw_data, dtype=np.dtype(\"uint8\"))\n",
    "        array = np.reshape(array, (image.height, image.width, 4))\n",
    "        array = array.astype(np.float32)\n",
    "        array = array[:, :, 0] * 1000.0 / 255.0  # Convert to meters\n",
    "        self.depth_array = array\n",
    "        self._depth_queue.put(array)\n",
    "\n",
    "    def _find_intersection_spawn(self, spawn_points):\n",
    "        \"\"\"Find a clear spawn point near an intersection\"\"\"\n",
    "        try:\n",
    "            # Shuffle spawn points for randomization\n",
    "            random.shuffle(spawn_points)\n",
    "            \n",
    "            for spawn_point in spawn_points:\n",
    "                # Get waypoint at spawn point\n",
    "                waypoint = self.world.get_map().get_waypoint(\n",
    "                    spawn_point.location,\n",
    "                    project_to_road=True,\n",
    "                    lane_type=carla.LaneType.Driving\n",
    "                )\n",
    "                \n",
    "                # Look for intersection\n",
    "                intersection_waypoint = waypoint\n",
    "                distance = 0\n",
    "                while not intersection_waypoint.is_intersection and distance < 100:\n",
    "                    # Get next waypoints\n",
    "                    next_waypoints = intersection_waypoint.next(5.0)\n",
    "                    if not next_waypoints:\n",
    "                        break\n",
    "                    intersection_waypoint = next_waypoints[0]\n",
    "                    distance += 5.0\n",
    "                \n",
    "                if intersection_waypoint.is_intersection:\n",
    "                    # Get spawn point before intersection\n",
    "                    spawn_transform = carla.Transform()\n",
    "                    prev_waypoint = intersection_waypoint.previous(20.0)[0]\n",
    "                    spawn_transform.location = prev_waypoint.transform.location\n",
    "                    spawn_transform.rotation = prev_waypoint.transform.rotation\n",
    "                    \n",
    "                    # Check if spawn point is clear\n",
    "                    if not self.world.cast_ray(\n",
    "                        spawn_transform.location,\n",
    "                        spawn_transform.location + carla.Location(z=2.0)\n",
    "                    ):\n",
    "                        return spawn_transform\n",
    "                        \n",
    "            # If no clear intersection spawn found, try regular spawn points\n",
    "            for spawn_point in spawn_points:\n",
    "                if not self.world.cast_ray(\n",
    "                    spawn_point.location,\n",
    "                    spawn_point.location + carla.Location(z=2.0)\n",
    "                ):\n",
    "                    return spawn_point\n",
    "                    \n",
    "            # If still no clear spawn found, return first spawn point\n",
    "            return spawn_points[0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding spawn point: {e}\")\n",
    "            return spawn_points[0]\n",
    "\n",
    "    def open(self, host='localhost', port=2000):\n",
    "        \"\"\"Initialize CARLA connection and setup vehicle with sensors.\"\"\"\n",
    "        try:\n",
    "            # Connect to CARLA\n",
    "            self.client = carla.Client(host, port)\n",
    "            self.client.set_timeout(20.0)  # Increased timeout for world loading\n",
    "\n",
    "            # Load Town03 which has good intersections and roads\n",
    "            self.client.load_world('Town02', reset_settings=True)\n",
    "            self.world = self.client.get_world()\n",
    "            print(\"World loaded successfully\")\n",
    "\n",
    "            # # Configure clear weather settings\n",
    "            # weather = carla.WeatherParameters(\n",
    "            #     cloudiness=0.0,\n",
    "            #     precipitation=0.0,\n",
    "            #     sun_altitude_angle=70.0,  # High sun for good lighting\n",
    "            # )\n",
    "\n",
    "            # Configure clear weather settings\n",
    "            weather = carla.WeatherParameters(\n",
    "                cloudiness=0.0,\n",
    "                precipitation=0.0,\n",
    "                precipitation_deposits=0.0,\n",
    "                wind_intensity=0.0,\n",
    "                sun_azimuth_angle=45.0,  # Position sun for optimal shadows\n",
    "                sun_altitude_angle=70.0,  # High sun for good lighting\n",
    "                fog_density=0.0,\n",
    "                fog_distance=0.0,\n",
    "                fog_falloff=0.0,\n",
    "                wetness=0.0,\n",
    "                scattering_intensity=0.0,\n",
    "                mie_scattering_scale=0.0,\n",
    "                rayleigh_scattering_scale=0.03310000151395798,\n",
    "                dust_storm=0.0\n",
    "            )\n",
    "            self.world.set_weather(weather)\n",
    "            print(\"World loaded successfully with clear weather\")\n",
    "\n",
    "\n",
    "            # Wait for world to stabilize\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Configure world settings\n",
    "            settings = self.world.get_settings()\n",
    "            settings.synchronous_mode = True\n",
    "            settings.fixed_delta_seconds = 0.05  # 20 FPS\n",
    "            settings.no_rendering_mode = False\n",
    "            self.world.apply_settings(settings)\n",
    "            print(\"World settings applied\")\n",
    "\n",
    "            # Initialize traffic manager\n",
    "            traffic_manager = self.client.get_trafficmanager()\n",
    "            traffic_manager.set_synchronous_mode(True)\n",
    "            traffic_manager.set_random_device_seed(0)  # for deterministic behavior\n",
    "            tm_port = traffic_manager.get_port()\n",
    "            \n",
    "            # Configure global traffic settings\n",
    "            traffic_manager.set_global_distance_to_leading_vehicle(3.0)  # meters\n",
    "            traffic_manager.global_percentage_speed_difference(10.0)  # % slower than speed limits\n",
    "            print(\"Traffic manager initialized\")\n",
    "\n",
    "            # Clear existing vehicles\n",
    "            self._clear_existing_actors()\n",
    "\n",
    "            # Get spawn points\n",
    "            spawn_points = self.world.get_map().get_spawn_points()\n",
    "            if not spawn_points:\n",
    "                raise RuntimeError(\"No spawn points found in map!\")\n",
    "\n",
    "            # Setup ego vehicle\n",
    "            blueprint_library = self.world.get_blueprint_library()\n",
    "            vehicle_bp = blueprint_library.find('vehicle.tesla.model3')\n",
    "            \n",
    "            # Set color\n",
    "            if vehicle_bp.has_attribute('color'):\n",
    "                color = random.choice(vehicle_bp.get_attribute('color').recommended_values)\n",
    "                vehicle_bp.set_attribute('color', color)\n",
    "\n",
    "            # Try to spawn vehicle at different points until successful\n",
    "            spawn_success = False\n",
    "            for spawn_point in spawn_points:\n",
    "                try:\n",
    "                    self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "                    if self.vehicle is not None:\n",
    "                        spawn_success = True\n",
    "                        print(f\"Vehicle spawned successfully at {spawn_point}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if not spawn_success:\n",
    "                raise RuntimeError(\"Failed to spawn vehicle after trying all spawn points\")\n",
    "\n",
    "            # Setup vehicle physics\n",
    "            physics_control = self.vehicle.get_physics_control()\n",
    "            physics_control.use_sweep_wheel_collision = True\n",
    "            self.vehicle.apply_physics_control(physics_control)\n",
    "\n",
    "            # Setup autopilot\n",
    "            self.vehicle.set_autopilot(True, tm_port)\n",
    "            \n",
    "            # Configure vehicle-specific behavior\n",
    "            traffic_manager.auto_lane_change(self.vehicle, True)\n",
    "            traffic_manager.set_desired_speed(self.vehicle, 30)  # 30 km/h\n",
    "            traffic_manager.distance_to_leading_vehicle(self.vehicle, 5.0)\n",
    "            traffic_manager.ignore_lights_percentage(self.vehicle, 0)\n",
    "            traffic_manager.ignore_signs_percentage(self.vehicle, 0)\n",
    "            traffic_manager.random_left_lanechange_percentage(self.vehicle, 0)\n",
    "            traffic_manager.random_right_lanechange_percentage(self.vehicle, 0)\n",
    "            traffic_manager.vehicle_percentage_speed_difference(self.vehicle, 0)  # Follow speed limit exactly\n",
    "\n",
    "            # Create sensor blueprints\n",
    "            camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "            depth_bp = blueprint_library.find('sensor.camera.depth')\n",
    "\n",
    "            # Configure camera settings\n",
    "            for bp in [camera_bp, depth_bp]:\n",
    "                bp.set_attribute('image_size_x', '800')\n",
    "                bp.set_attribute('image_size_y', '600')\n",
    "                bp.set_attribute('fov', '90')\n",
    "                bp.set_attribute('sensor_tick', '0.0')\n",
    "\n",
    "            # Setup camera transform for driver view\n",
    "            camera_transform = carla.Transform(\n",
    "                carla.Location(x=0.8, y=0.0, z=1.7),  # Position just behind windshield\n",
    "                carla.Rotation(pitch=-5.0)  # Look slightly down\n",
    "            )\n",
    "\n",
    "            # Spawn cameras\n",
    "            self.camera_rgb = self.world.spawn_actor(\n",
    "                camera_bp,\n",
    "                camera_transform,\n",
    "                attach_to=self.vehicle,\n",
    "                attachment_type=carla.AttachmentType.Rigid\n",
    "            )\n",
    "\n",
    "            self.camera_depth = self.world.spawn_actor(\n",
    "                depth_bp,\n",
    "                camera_transform,\n",
    "                attach_to=self.vehicle,\n",
    "                attachment_type=carla.AttachmentType.Rigid\n",
    "            )\n",
    "\n",
    "            # Setup sensor callbacks\n",
    "            self.camera_rgb.listen(self.image_callback)\n",
    "            self.camera_depth.listen(self.depth_callback)\n",
    "            print(\"Cameras initialized\")\n",
    "\n",
    "            # Spawn other vehicles\n",
    "            num_vehicles = 20\n",
    "            self._spawn_npcs(num_vehicles=num_vehicles, num_pedestrians=10)\n",
    "            print(f\"Spawned {num_vehicles} NPC vehicles\")\n",
    "\n",
    "            # Move spectator to follow vehicle\n",
    "            self.spectator = self.world.get_spectator()\n",
    "            self._update_spectator()  # Initial update\n",
    "            transform = self.vehicle.get_transform()\n",
    "            self.spectator.set_transform(carla.Transform(\n",
    "                transform.location + carla.Location(z=50),\n",
    "                carla.Rotation(pitch=-90)\n",
    "            ))\n",
    "\n",
    "            # Wait for sensors to initialize\n",
    "            time.sleep(1)\n",
    "\n",
    "            print(\"CARLA camera system initialized successfully\")\n",
    "            return True\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Failed to connect to CARLA: {error}\")\n",
    "            self._cleanup_actors()\n",
    "            return False\n",
    "        \n",
    "    def _update_spectator(self):\n",
    "        \"\"\"Update spectator to follow vehicle from behind\"\"\"\n",
    "        if self.vehicle and self.spectator:\n",
    "            # Get vehicle's transform\n",
    "            vehicle_transform = self.vehicle.get_transform()\n",
    "            \n",
    "            # Calculate camera position behind and above vehicle\n",
    "            camera_distance = 10  # Distance behind the vehicle\n",
    "            camera_height = 5    # Height above the vehicle\n",
    "            \n",
    "            # Get vehicle's forward vector\n",
    "            forward_vector = vehicle_transform.get_forward_vector()\n",
    "            \n",
    "            # Calculate camera position\n",
    "            camera_location = vehicle_transform.location - forward_vector * camera_distance\n",
    "            camera_location.z = vehicle_transform.location.z + camera_height\n",
    "            \n",
    "            # Calculate camera rotation to look at vehicle\n",
    "            camera_rotation = vehicle_transform.rotation\n",
    "            camera_rotation.pitch = -15  # Look down slightly\n",
    "            \n",
    "            # Set spectator transform\n",
    "            self.spectator.set_transform(\n",
    "                carla.Transform(camera_location, camera_rotation)\n",
    "            )\n",
    "\n",
    "    def _clear_existing_actors(self):\n",
    "        \"\"\"Clear all existing actors from the world.\"\"\"\n",
    "        try:\n",
    "            actor_list = self.world.get_actors()\n",
    "            for actor in actor_list:\n",
    "                if actor.type_id.startswith(('vehicle', 'sensor', 'walker')):\n",
    "                    actor.destroy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing actors: {e}\")\n",
    "\n",
    "    def _cleanup_actors(self):\n",
    "        \"\"\"Clean up actors on error.\"\"\"\n",
    "        try:\n",
    "            if hasattr(self, 'camera_rgb') and self.camera_rgb:\n",
    "                self.camera_rgb.destroy()\n",
    "            if hasattr(self, 'camera_depth') and self.camera_depth:\n",
    "                self.camera_depth.destroy()\n",
    "            if hasattr(self, 'vehicle') and self.vehicle:\n",
    "                self.vehicle.destroy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during actor cleanup: {e}\")\n",
    "            \n",
    "    def _spawn_npcs(self, num_vehicles=10, num_pedestrians=5):\n",
    "        \"\"\"Spawn NPC vehicles and pedestrians\"\"\"\n",
    "        try:\n",
    "            # Spawn NPC vehicles\n",
    "            blueprint_library = self.world.get_blueprint_library()\n",
    "            car_blueprints = [bp for bp in blueprint_library.filter('vehicle.*')\n",
    "                            if int(bp.get_attribute('number_of_wheels')) == 4]\n",
    "            \n",
    "            spawn_points = self.world.get_map().get_spawn_points()\n",
    "            for _ in range(min(num_vehicles, len(spawn_points))):\n",
    "                spawn_point = random.choice(spawn_points)\n",
    "                bp = random.choice(car_blueprints)\n",
    "                npc = self.world.spawn_actor(bp, spawn_point)\n",
    "                if npc:\n",
    "                    npc.set_autopilot(True)\n",
    "                    spawn_points.remove(spawn_point)\n",
    "            \n",
    "            # Spawn pedestrians\n",
    "            pedestrian_bps = blueprint_library.filter('walker.pedestrian.*')\n",
    "            for _ in range(num_pedestrians):\n",
    "                bp = random.choice(pedestrian_bps)\n",
    "                spawn_point = carla.Transform(\n",
    "                    self.world.get_random_location_from_navigation()\n",
    "                )\n",
    "                pedestrian = self.world.spawn_actor(bp, spawn_point)\n",
    "                if pedestrian:\n",
    "                    # Add pedestrian controller\n",
    "                    controller_bp = blueprint_library.find('controller.ai.walker')\n",
    "                    controller = self.world.spawn_actor(controller_bp, carla.Transform(), attach_to=pedestrian)\n",
    "                    controller.start()\n",
    "                    controller.go_to_location(self.world.get_random_location_from_navigation())\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error spawning NPCs: {e}\")\n",
    "\n",
    "    def grab_frame(self):\n",
    "        try:\n",
    "            # Update spectator position before ticking the world\n",
    "            self._update_spectator()\n",
    "            \n",
    "            self.world.tick()\n",
    "            try:\n",
    "                image = self._image_queue.get(timeout=2.0)\n",
    "                depth = self._depth_queue.get(timeout=2.0)\n",
    "                return image, depth\n",
    "            except queue.Empty:\n",
    "                print(\"Timeout waiting for sensor data\")\n",
    "                return None, None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error grabbing frame: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "    # In CARLACamera class, add after vehicle spawn:\n",
    "    def _setup_vehicle_behavior(self):\n",
    "        \"\"\"Setup automatic driving behavior\"\"\"\n",
    "        # Enable autopilot\n",
    "        self.vehicle.set_autopilot(True)\n",
    "        \n",
    "        # Set high-level behavior parameters\n",
    "        traffic_manager = self.client.get_trafficmanager()\n",
    "        traffic_manager.global_percentage_speed_difference(10.0)  # Drive 10% slower than speed limit\n",
    "        traffic_manager.set_synchronous_mode(True)\n",
    "        \n",
    "        # Set vehicle-specific behavior\n",
    "        tm_port = traffic_manager.get_port()\n",
    "        self.vehicle.set_autopilot(True, tm_port)\n",
    "        \n",
    "        # Configure vehicle behavior\n",
    "        traffic_manager.vehicle_percentage_speed_difference(self.vehicle, 0)\n",
    "        traffic_manager.set_desired_speed(self.vehicle, 30)  # Set desired speed to 30 km/h\n",
    "        traffic_manager.distance_to_leading_vehicle(self.vehicle, 5.0)  # Set safe distance\n",
    "        traffic_manager.ignore_lights_percentage(self.vehicle, 0)  # Always obey traffic lights\n",
    "        traffic_manager.ignore_signs_percentage(self.vehicle, 0)  # Always obey signs\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            if self.camera_rgb:\n",
    "                self.camera_rgb.stop()\n",
    "                self.camera_rgb.destroy()\n",
    "            if self.camera_depth:\n",
    "                self.camera_depth.stop()\n",
    "                self.camera_depth.destroy()\n",
    "            if self.vehicle:\n",
    "                self.vehicle.set_autopilot(False)\n",
    "                self.vehicle.destroy()\n",
    "            \n",
    "            if self.world:\n",
    "                settings = self.world.get_settings()\n",
    "                settings.synchronous_mode = False\n",
    "                self.world.apply_settings(settings)\n",
    "            \n",
    "            print(\"Closed CARLA connection\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkManager:\n",
    "    def __init__(self):\n",
    "        self.distance_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.video_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.track_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.gui_distance_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        \n",
    "        self.distance_address = ('localhost', 12345)\n",
    "        self.gui_distance_address = ('localhost', 12348)\n",
    "        self.video_address = ('localhost', 12347)\n",
    "        self.track_address = ('localhost', 12349)\n",
    "\n",
    "    def send_frame(self, frame, fps):\n",
    "        try:\n",
    "            if frame is None or frame.size == 0:\n",
    "                print(\"Error: Invalid frame for sending\")\n",
    "                return\n",
    "                \n",
    "            fps_bytes = struct.pack('f', fps)\n",
    "            self.video_socket.sendto(fps_bytes, self.video_address)\n",
    "                \n",
    "            _, encoded_frame = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 80])\n",
    "            frame_bytes = encoded_frame.tobytes()\n",
    "            \n",
    "            size_bytes = struct.pack('I', len(frame_bytes))\n",
    "            self.video_socket.sendto(size_bytes, self.video_address)\n",
    "            \n",
    "            chunk_size = 8192\n",
    "            for i in range(0, len(frame_bytes), chunk_size):\n",
    "                chunk = frame_bytes[i:i + chunk_size]\n",
    "                self.video_socket.sendto(chunk, self.video_address)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error sending frame: {e}\")\n",
    "\n",
    "    def send_distance(self, distance):\n",
    "        try:\n",
    "            distance_bytes = struct.pack('f', distance)\n",
    "            self.distance_socket.sendto(distance_bytes, self.distance_address)\n",
    "            self.gui_distance_socket.sendto(distance_bytes, self.gui_distance_address)\n",
    "        except Exception as e:\n",
    "            print(f\"Error sending distance: {e}\")\n",
    "\n",
    "    def send_tracked_objects(self, tracked_objects, depth_array):\n",
    "        try:\n",
    "            tracked_objects_data = []\n",
    "            for obj in tracked_objects:\n",
    "                x1, y1, x2, y2, track_id = obj\n",
    "                cx = int((x1 + x2) // 2)\n",
    "                cy = int(y2)\n",
    "                \n",
    "                if 0 <= cx < depth_array.shape[1] and 0 <= cy < depth_array.shape[0]:\n",
    "                    depth = depth_array[cy, cx]\n",
    "                    if np.isfinite(depth):\n",
    "                        tracked_objects_data.append([cx, cy, depth, track_id])\n",
    "\n",
    "            if tracked_objects_data:\n",
    "                data_array = np.array(tracked_objects_data, dtype=np.float32)\n",
    "                data_bytes = data_array.tobytes()\n",
    "                \n",
    "                size_bytes = struct.pack('I', len(data_bytes))\n",
    "                self.track_socket.sendto(size_bytes, self.track_address)\n",
    "                \n",
    "                chunk_size = 8192\n",
    "                for i in range(0, len(data_bytes), chunk_size):\n",
    "                    chunk = data_bytes[i:i + chunk_size]\n",
    "                    self.track_socket.sendto(chunk, self.track_address)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error sending tracked objects: {e}\")\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.distance_socket.close()\n",
    "        self.video_socket.close()\n",
    "        self.track_socket.close()\n",
    "        self.gui_distance_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lane_distances(da_seg_mask, ll_seg_mask, image_width):\n",
    "    \"\"\"\n",
    "    Calculate distances from center to left and right lane boundaries\n",
    "    \n",
    "    Args:\n",
    "        da_seg_mask: Driving area segmentation mask\n",
    "        ll_seg_mask: Lane line segmentation mask\n",
    "        image_width: Width of the input image\n",
    "    \n",
    "    Returns:\n",
    "        left_distance: Distance from center to left lane boundary in pixels\n",
    "        right_distance: Distance from center to right lane boundary in pixels\n",
    "    \"\"\"\n",
    "    # Get the bottom row of the masks for distance calculation\n",
    "    bottom_row_idx = -50  # Look slightly above the bottom edge for more stable measurements\n",
    "    da_bottom = da_seg_mask[bottom_row_idx, :]\n",
    "    ll_bottom = ll_seg_mask[bottom_row_idx, :]\n",
    "    \n",
    "    # Find center point\n",
    "    center_x = image_width // 2\n",
    "    \n",
    "    # Find left and right boundaries\n",
    "    # First check lane lines\n",
    "    left_points = np.where(ll_bottom[:center_x] > 0)[0]\n",
    "    right_points = np.where(ll_bottom[center_x:] > 0)[0]\n",
    "    \n",
    "    # If lane lines not found, use driving area boundaries\n",
    "    if len(left_points) == 0:\n",
    "        left_points = np.where(da_bottom[:center_x] > 0)[0]\n",
    "    if len(right_points) == 0:\n",
    "        right_points = np.where(da_bottom[center_x:] > 0)[0]\n",
    "    \n",
    "    # Calculate distances\n",
    "    left_distance = center_x - left_points[-1] if len(left_points) > 0 else None\n",
    "    right_distance = right_points[0] if len(right_points) > 0 else None\n",
    "    \n",
    "    if right_distance is not None:\n",
    "        right_distance += center_x\n",
    "    \n",
    "    return left_distance, right_distance\n",
    "\n",
    "def visualize_lane_distances(image, left_distance, right_distance, pixels_to_meters=0.01):\n",
    "    \"\"\"\n",
    "    Visualize the lane distances on the image\n",
    "    Args:\n",
    "        image: Input image\n",
    "        left_distance: Distance to left lane in pixels\n",
    "        right_distance: Distance to right lane in pixels\n",
    "        pixels_to_meters: Conversion factor from pixels to meters (approximate)\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    center_x = width // 2\n",
    "    y_position = height - 50  # Match the measurement position\n",
    "    \n",
    "    # Draw center point\n",
    "    cv2.circle(image, (center_x, y_position), 5, (0, 255, 0), -1)\n",
    "    \n",
    "    # Draw distances if available\n",
    "    if left_distance is not None:\n",
    "        left_x = center_x - left_distance\n",
    "        cv2.line(image, (center_x, y_position), (left_x, y_position), (255, 0, 0), 2)\n",
    "        cv2.putText(image, f'{left_distance:.2f}px', (left_x, y_position - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    if right_distance is not None:\n",
    "        right_x = right_distance\n",
    "        cv2.line(image, (center_x, y_position), (right_x, y_position), (0, 0, 255), 2)\n",
    "        cv2.putText(image, f'{right_distance:.2f}px', (right_x, y_position - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Ensure img is in the correct format\n",
    "    if len(img.shape) == 2:  # If grayscale, add channel dimension\n",
    "        img = np.stack((img,) * 3, axis=-1)\n",
    "    elif len(img.shape) == 4:  # If RGBA, convert to RGB\n",
    "        img = img[:, :, :3]\n",
    "        \n",
    "    shape = img.shape[:2]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "    \n",
    "    if auto:\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)\n",
    "    elif scaleFill:\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "\n",
    "    dw /= 2\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def driving_area_mask_numpy(seg_array):\n",
    "    \"\"\"\n",
    "    Create driving area mask from segmentation output (numpy version).\n",
    "    \n",
    "    Args:\n",
    "        seg_array: numpy array of segmentation output\n",
    "        \n",
    "    Returns:\n",
    "        Binary mask of driving area\n",
    "    \"\"\"\n",
    "    # Convert 3D array to 2D by taking argmax along last axis if needed\n",
    "    if len(seg_array.shape) == 3:\n",
    "        seg_array = np.argmax(seg_array, axis=0)\n",
    "    elif len(seg_array.shape) == 4:\n",
    "        seg_array = np.argmax(seg_array[0], axis=0)\n",
    "        \n",
    "    # Create binary mask where class 1 is driving area\n",
    "    mask = (seg_array == 1).astype(np.uint8) * 255\n",
    "    return mask\n",
    "\n",
    "def lane_line_mask_numpy(ll_array):\n",
    "    \"\"\"\n",
    "    Create lane line mask from line detection output (numpy version).\n",
    "    \n",
    "    Args:\n",
    "        ll_array: numpy array of lane line detection output\n",
    "        \n",
    "    Returns:\n",
    "        Binary mask of lane lines\n",
    "    \"\"\"\n",
    "    # Handle different input shapes\n",
    "    if len(ll_array.shape) == 4:\n",
    "        ll_array = ll_array[0]\n",
    "    elif len(ll_array.shape) == 3:\n",
    "        pass  # Already in correct format\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected shape for lane line array: {ll_array.shape}\")\n",
    "        \n",
    "    # If array has multiple channels, take the most confident one\n",
    "    if ll_array.shape[0] > 1:\n",
    "        ll_array = np.argmax(ll_array, axis=0)\n",
    "    else:\n",
    "        ll_array = ll_array[0] > 0.5  # Threshold if single channel\n",
    "        \n",
    "    # Create binary mask\n",
    "    mask = ll_array.astype(np.uint8) * 255\n",
    "    return mask\n",
    "\n",
    "def process_segmentation(seg, ll, im0, img):\n",
    "    \"\"\"\n",
    "    Process segmentation masks with proper alignment and scaling.\n",
    "    \n",
    "    Args:\n",
    "        seg: Segmentation output (tensor or numpy array)\n",
    "        ll: Lane line output (tensor or numpy array)\n",
    "        im0: Original image\n",
    "        img: Preprocessed image tensor\n",
    "    \n",
    "    Returns:\n",
    "        da_seg_mask, ll_seg_mask: Properly aligned segmentation masks\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy if needed\n",
    "    if torch.is_tensor(seg):\n",
    "        seg = seg.cpu().numpy()\n",
    "    if torch.is_tensor(ll):\n",
    "        ll = ll.cpu().numpy()\n",
    "    if torch.is_tensor(img):\n",
    "        img = img.cpu().numpy()\n",
    "    \n",
    "    # Get original and preprocessed dimensions\n",
    "    orig_h, orig_w = im0.shape[:2]\n",
    "    \n",
    "    # Get preprocessed dimensions\n",
    "    if len(img.shape) == 4:\n",
    "        lb_h, lb_w = img.shape[2:]\n",
    "    else:\n",
    "        lb_h, lb_w = img.shape[:2]\n",
    "    \n",
    "    # Process masks using numpy versions of mask functions\n",
    "    da_seg_mask = driving_area_mask_numpy(seg)\n",
    "    ll_seg_mask = lane_line_mask_numpy(ll)\n",
    "\n",
    "    # Resize masks to match original image dimensions\n",
    "    if da_seg_mask.shape[:2] != (orig_h, orig_w):\n",
    "        da_seg_mask = cv2.resize(da_seg_mask, (orig_w, orig_h), \n",
    "                                interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    if ll_seg_mask.shape[:2] != (orig_h, orig_w):\n",
    "        ll_seg_mask = cv2.resize(ll_seg_mask, (orig_w, orig_h), \n",
    "                                interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return da_seg_mask, ll_seg_mask\n",
    "\n",
    "def apply_segmentation_overlay(frame, da_seg_mask, ll_seg_mask):\n",
    "    \"\"\"\n",
    "    Apply segmentation overlays to frame with proper blending and colors.\n",
    "    \"\"\"\n",
    "    result_img = frame.copy()\n",
    "    \n",
    "    # Apply driving area overlay (light blue)\n",
    "    if da_seg_mask.any():\n",
    "        da_overlay = np.zeros_like(frame)\n",
    "        da_overlay[da_seg_mask > 0] = [0, 0, 255]  # BGR light blue\n",
    "        # Use lower alpha for more subtle blend\n",
    "        result_img = cv2.addWeighted(result_img, 0.7, da_overlay, 1.0, 0)\n",
    "    \n",
    "    # Apply lane line overlay (red)\n",
    "    if ll_seg_mask.any():\n",
    "        ll_overlay = np.zeros_like(frame)\n",
    "        ll_overlay[ll_seg_mask > 0] = [255, 0, 0]  # BGR red\n",
    "        # Use higher alpha for more visible lane lines\n",
    "        result_img = cv2.addWeighted(result_img, 0.7, ll_overlay, 1.0, 0)\n",
    "    \n",
    "    return result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(\n",
    "    weights='data/weights/yolopv2.pt',\n",
    "    img_size=640,\n",
    "    conf_thres=0.3,\n",
    "    iou_thres=0.45,\n",
    "    device='0',\n",
    "    host='localhost',\n",
    "    port=2000,\n",
    "    show_vid=False\n",
    "):\n",
    "    print(\"\\nInitializing components...\")\n",
    "\n",
    "    # Initialize models\n",
    "    try:\n",
    "        device = select_device(device)\n",
    "        model = torch.jit.load(weights).to(device)\n",
    "        person_model = YOLO('yolov8n.pt')\n",
    "        person_model.overrides['classes'] = [0, 4]  # person, car, motorcycle\n",
    "        # Initialize waypoint extractor\n",
    "        waypoint_extractor = WaypointExtractor(num_points=6)\n",
    "\n",
    "        half = device.type != 'cpu'\n",
    "        if half:\n",
    "            model.half()\n",
    "        model.eval()\n",
    "        print(\"Models loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing models: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize CARLA camera and network\n",
    "    try:\n",
    "        camera = CARLACamera()\n",
    "        if not camera.open(host, port):\n",
    "            print(\"Failed to initialize CARLA camera\")\n",
    "            return\n",
    "\n",
    "        network = NetworkManager()\n",
    "        print(\"Camera and network initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing camera/network: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize trackers\n",
    "    vehicle_tracker = Sort(max_age=10, min_hits=3, iou_threshold=0.45)\n",
    "    person_tracker = Sort(max_age=10, min_hits=3, iou_threshold=0.45)\n",
    "    \n",
    "    # Performance monitoring\n",
    "    fps_avg = AverageMeter()\n",
    "    frame_count = 0\n",
    "\n",
    "    # Create visualization window\n",
    "    if show_vid:\n",
    "        cv2.namedWindow('CARLA Detection', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('CARLA Detection', 1280, 720)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nStarting detection loop...\")\n",
    "        while True:\n",
    "            try:\n",
    "                # Get frame from CARLA\n",
    "                im0, depth = camera.grab_frame()\n",
    "                if im0 is None or depth is None:\n",
    "                    print(\"Error: Failed to grab frame\")\n",
    "                    time.sleep(0.1)\n",
    "                    continue\n",
    "\n",
    "                frame_count += 1\n",
    "                if frame_count % 30 == 0:  # Report FPS every 30 frames\n",
    "                    print(f\"Processed {frame_count} frames, FPS: {fps_avg.avg:.1f}\")\n",
    "\n",
    "                result_img = im0.copy()\n",
    "\n",
    "                # Image preprocessing\n",
    "                im0_rgb = cv2.cvtColor(im0, cv2.COLOR_BGR2RGB)\n",
    "                img = letterbox(im0_rgb, new_shape=img_size)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                img = torch.from_numpy(img).to(device).float()\n",
    "                img /= 255.0\n",
    "                img = img.permute(2, 0, 1).unsqueeze(0)\n",
    "                \n",
    "                if half:\n",
    "                    img = img.half()\n",
    "\n",
    "                # Run inference\n",
    "                t1 = time_synchronized()\n",
    "                with torch.no_grad():\n",
    "                    # YOLOPv2 inference\n",
    "                    [pred, anchor_grid], seg, ll = model(img)\n",
    "                    \n",
    "                    # YOLOv8 inference for person detection\n",
    "                    person_results = person_model.predict(im0, conf=conf_thres, iou=iou_thres)\n",
    "                \n",
    "                t2 = time_synchronized()\n",
    "                fps = 1 / (t2 - t1)\n",
    "                fps_avg.update(fps)\n",
    "\n",
    "                # Process YOLOPv2 predictions\n",
    "                pred = split_for_trace_model(pred, anchor_grid)\n",
    "                pred = non_max_suppression(pred, conf_thres, iou_thres)\n",
    "\n",
    "                # Rest of the code remains the same...\n",
    "                # (Processing detections, updating trackers, drawing results, etc.)\n",
    "                \n",
    "                # Process detections\n",
    "                vehicle_detections = []\n",
    "                person_detections = []\n",
    "\n",
    "                # Process vehicle detections\n",
    "                for det in pred:\n",
    "                    if len(det):\n",
    "                        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                        for *xyxy, conf, cls in det.cpu().numpy():\n",
    "                            vehicle_detections.append([xyxy[0], xyxy[1], xyxy[2], xyxy[3], conf])\n",
    "\n",
    "                # Process person detections\n",
    "                if len(person_results) > 0:\n",
    "                    for result in person_results:\n",
    "                        boxes = result.boxes\n",
    "                        for box in boxes:\n",
    "                            xyxy = box.xyxy[0].cpu().numpy()\n",
    "                            conf = box.conf[0].cpu().numpy()\n",
    "                            person_detections.append([xyxy[0], xyxy[1], xyxy[2], xyxy[3], conf])\n",
    "\n",
    "                # Update trackers\n",
    "                tracked_vehicles = vehicle_tracker.update(\n",
    "                    np.array(vehicle_detections) if vehicle_detections else np.empty((0, 5))\n",
    "                )\n",
    "                tracked_persons = person_tracker.update(\n",
    "                    np.array(person_detections) if person_detections else np.empty((0, 5))\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    # Get segmentation masks\n",
    "                    da_seg_mask, ll_seg_mask = process_segmentation(seg, ll, im0, img)\n",
    "\n",
    "                    # Extract and visualize waypoints\n",
    "                    waypoints = waypoint_extractor.extract_waypoints(\n",
    "                        da_seg_mask, \n",
    "                        depth,\n",
    "                        im0.shape[0],\n",
    "                        im0.shape[1]\n",
    "                    )\n",
    "\n",
    "                    # Visualize waypoints on the result image\n",
    "                    result_img = waypoint_extractor.visualize_waypoints(result_img, waypoints)\n",
    "                    # Calculate lane distances\n",
    "                    left_dist, right_dist = calculate_lane_distances(da_seg_mask, ll_seg_mask, im0.shape[1])\n",
    "\n",
    "                    # Visualize the distances\n",
    "                    result_img = visualize_lane_distances(result_img, left_dist, right_dist)\n",
    "    \n",
    "                    \n",
    "                    # Apply segmentation overlay\n",
    "                    result_img = apply_segmentation_overlay(result_img, da_seg_mask, ll_seg_mask)\n",
    "                    result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in segmentation processing: {e}\")\n",
    "                    print(f\"Segmentation shape: {seg.shape if hasattr(seg, 'shape') else 'None'}\")\n",
    "                    print(f\"Lane line shape: {ll.shape if hasattr(ll, 'shape') else 'None'}\")\n",
    "                    print(f\"Image shape: {im0.shape}\")\n",
    "                \n",
    "                # Draw detections\n",
    "                try:\n",
    "                    # Draw vehicle boxes\n",
    "                    for obj in tracked_vehicles:\n",
    "                        x1, y1, x2, y2, track_id = map(int, obj)\n",
    "                        cv2.rectangle(result_img, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                        cv2.putText(result_img, f'Vehicle {int(track_id)}', (x1, y1-10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "\n",
    "                    # Draw person boxes\n",
    "                    for obj in tracked_persons:\n",
    "                        x1, y1, x2, y2, track_id = map(int, obj)\n",
    "                        cv2.rectangle(result_img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                        cv2.putText(result_img, f'Person {int(track_id)}', (x1, y1-10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "                    # Add FPS display\n",
    "                    cv2.putText(result_img, f'FPS: {fps_avg.avg:.1f}', (20, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in drawing detections: {e}\")\n",
    "\n",
    "                # Calculate distances and send data\n",
    "                try:\n",
    "                    # Combine tracked objects\n",
    "                    all_tracked_objects = np.concatenate((tracked_vehicles, tracked_persons)) \\\n",
    "                        if len(tracked_vehicles) > 0 and len(tracked_persons) > 0 \\\n",
    "                        else tracked_vehicles if len(tracked_vehicles) > 0 \\\n",
    "                        else tracked_persons if len(tracked_persons) > 0 \\\n",
    "                        else np.array([])\n",
    "\n",
    "                    # Find minimum distance\n",
    "                    min_distance = float('inf')\n",
    "                    for obj in all_tracked_objects:\n",
    "                        x1, y1, x2, y2, _ = obj\n",
    "                        cx = int((x1 + x2) // 2)\n",
    "                        cy = int(y2)\n",
    "                        if 0 <= cx < depth.shape[1] and 0 <= cy < depth.shape[0]:\n",
    "                            dist = depth[cy, cx]\n",
    "                            if np.isfinite(dist):\n",
    "                                min_distance = min(min_distance, dist)\n",
    "\n",
    "                    if min_distance != float('inf'):\n",
    "                        network.send_distance(min_distance)\n",
    "\n",
    "                    # Send visualization and tracking data\n",
    "                    if result_img is not None and result_img.size > 0:\n",
    "                        cv2.putText(result_img, f'Min Distance: {min_distance:.2f}m', (20, 60),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                        network.send_frame(result_img, fps_avg.avg)\n",
    "                    network.send_tracked_objects(all_tracked_objects, depth)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in distance calculation/data transmission: {e}\")\n",
    "\n",
    "                # Show results\n",
    "                if show_vid:\n",
    "                    cv2.imshow('CARLA Detection', result_img)\n",
    "\n",
    "                # Check for exit condition\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    print(\"\\nDetection stopped by user\")\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in main detection loop: {e}\")\n",
    "                continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nDetection stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in detection: {e}\")\n",
    "    finally:\n",
    "        print(\"\\nCleaning up...\")\n",
    "        try:\n",
    "            camera.close()\n",
    "            network.cleanup()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"Cleanup completed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"\\nStarting CARLA Vision Detection System...\")\n",
    "    print(\"Initializing CARLA client and loading models...\")\n",
    "    with torch.no_grad():\n",
    "        detect(show_vid=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nDetection stopped by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in main execution: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    print(\"\\nProgram terminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zed_sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
